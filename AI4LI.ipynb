{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline for processing Vietnamese text"
      ],
      "metadata": {
        "id": "97ti2g6sDT8p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j5TVuHZFBMa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcbf9844-d521-41fd-d1a7-58aae67daaba",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from underthesea) (8.2.1)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from underthesea) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from underthesea) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from underthesea) (2.32.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.5.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.6.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from underthesea) (6.0.2)\n",
            "Collecting underthesea-core==1.0.4 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.4-cp311-cp311-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->underthesea) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2025.7.14)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (3.6.0)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading underthesea_core-1.0.4-cp311-cp311-manylinux2010_x86_64.whl (657 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: underthesea-core, unidecode, python-crfsuite, underthesea, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0 underthesea-6.8.4 underthesea-core-1.0.4 unidecode-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install underthesea pyvi unidecode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from underthesea import pos_tag, word_tokenize\n",
        "from pyvi import ViTokenizer\n",
        "from unidecode import unidecode"
      ],
      "metadata": {
        "id": "zcbZ7Pvwm4x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess input text"
      ],
      "metadata": {
        "id": "ge48aGzBnZ4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = re.sub(r\"[^a-zA-ZÀ-ỹà-ỹ\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "H1euNmDonY8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle diacritic"
      ],
      "metadata": {
        "id": "CvGD4iFRnckr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_diacritics(text):\n",
        "    return unidecode(text)"
      ],
      "metadata": {
        "id": "Zpb0dR_1neJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization with many options"
      ],
      "metadata": {
        "id": "Zfw3T6ZzniZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text, method='underthesea'):\n",
        "    if method == 'underthesea':\n",
        "        return word_tokenize(text)\n",
        "    elif method == 'pyvi':\n",
        "        return ViTokenizer.tokenize(text).split()\n",
        "    elif method == 'naive':\n",
        "        return text.split()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported tokenization method\")"
      ],
      "metadata": {
        "id": "uvDa8CrfnkFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS tagging"
      ],
      "metadata": {
        "id": "ufvnUSvKn79V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tagging(tokens, method='underthesea'):\n",
        "    if method == 'underthesea':\n",
        "        return pos_tag(' '.join(tokens))\n",
        "    else:\n",
        "        raise ValueError(\"Only underthesea supported for POS tagging now\")"
      ],
      "metadata": {
        "id": "9qm1fn3jn7MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full pipeline"
      ],
      "metadata": {
        "id": "ISdt06gRoCnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(text):\n",
        "    print(\"Original:\", text)\n",
        "    preprocessed = preprocess(text)\n",
        "    print(\"Preprocessed:\", preprocessed)\n",
        "    normalized = remove_diacritics(preprocessed)\n",
        "    print(\"Diacritics:\", normalized)\n",
        "    tokens = tokenize(preprocessed)\n",
        "    print(\"Tokens:\", tokens)\n",
        "    pos_tags = pos_tag(preprocessed)\n",
        "    print(\"POS Tags:\", pos_tags)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "9wgw0PpHotfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "cKPxkPapoGe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"Xin chào! Đây là một ví dụ đơn giản. Bạn có khỏe không?\",\n",
        "    \"Tôi thích ăn phở, bún bò Huế, và cả mì Quảng nữa!!!\",\n",
        "    \"Công nghệ AI đang phát triển rất nhanh; đặc biệt là trong lĩnh vực y tế & giáo dục.\",\n",
        "    \"‘Tôi đã nói với anh ta: “Đừng làm vậy nữa!”,’ cô ấy kể lại.\",\n",
        "    \"Dữ liệu đến từ nhiều nguồn: báo chí, mạng xã hội, và thậm chí cả tin đồn.\",\n",
        "    \"Hệ thống này hoạt động tốt ở cả thành thị lẫn nông thôn - điều này rất quan trọng.\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    run_pipeline(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvfbmNp2oHL_",
        "outputId": "31acfe55-5d5c-4ee0-eb68-a462d6617e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Xin chào! Đây là một ví dụ đơn giản. Bạn có khỏe không?\n",
            "Preprocessed: Xin chào Đây là một ví dụ đơn giản Bạn có khỏe không\n",
            "Diacritics: Xin chao Day la mot vi du don gian Ban co khoe khong\n",
            "Tokens: ['Xin', 'chào Đây', 'là', 'một', 'ví dụ', 'đơn giản', 'Bạn', 'có', 'khỏe', 'không']\n",
            "POS Tags: [('Xin', 'V'), ('chào Đây', 'V'), ('là', 'V'), ('một', 'M'), ('ví dụ', 'N'), ('đơn giản', 'A'), ('Bạn', 'N'), ('có', 'V'), ('khỏe', 'N'), ('không', 'R')]\n",
            "--------------------------------------------------\n",
            "Original: Tôi thích ăn phở, bún bò Huế, và cả mì Quảng nữa!!!\n",
            "Preprocessed: Tôi thích ăn phở bún bò Huế và cả mì Quảng nữa\n",
            "Diacritics: Toi thich an pho bun bo Hue va ca mi Quang nua\n",
            "Tokens: ['Tôi', 'thích', 'ăn', 'phở', 'bún bò', 'Huế', 'và', 'cả', 'mì', 'Quảng nữa']\n",
            "POS Tags: [('Tôi', 'P'), ('thích', 'V'), ('ăn', 'V'), ('phở', 'N'), ('bún bò', 'N'), ('Huế', 'Np'), ('và', 'C'), ('cả', 'T'), ('mì', 'N'), ('Quảng nữa', 'N')]\n",
            "--------------------------------------------------\n",
            "Original: Công nghệ AI đang phát triển rất nhanh; đặc biệt là trong lĩnh vực y tế & giáo dục.\n",
            "Preprocessed: Công nghệ AI đang phát triển rất nhanh đặc biệt là trong lĩnh vực y tế giáo dục\n",
            "Diacritics: Cong nghe AI dang phat trien rat nhanh dac biet la trong linh vuc y te giao duc\n",
            "Tokens: ['Công nghệ', 'AI', 'đang', 'phát triển', 'rất', 'nhanh', 'đặc biệt', 'là', 'trong', 'lĩnh vực', 'y tế', 'giáo dục']\n",
            "POS Tags: [('Công nghệ', 'N'), ('AI', 'P'), ('đang', 'R'), ('phát triển', 'V'), ('rất', 'R'), ('nhanh', 'A'), ('đặc biệt', 'A'), ('là', 'C'), ('trong', 'E'), ('lĩnh vực', 'N'), ('y tế', 'N'), ('giáo dục', 'V')]\n",
            "--------------------------------------------------\n",
            "Original: ‘Tôi đã nói với anh ta: “Đừng làm vậy nữa!”,’ cô ấy kể lại.\n",
            "Preprocessed: Tôi đã nói với anh ta Đừng làm vậy nữa cô ấy kể lại\n",
            "Diacritics: Toi da noi voi anh ta Dung lam vay nua co ay ke lai\n",
            "Tokens: ['Tôi', 'đã', 'nói', 'với', 'anh', 'ta', 'Đừng', 'làm vậy', 'nữa', 'cô', 'ấy', 'kể', 'lại']\n",
            "POS Tags: [('Tôi', 'P'), ('đã', 'R'), ('nói', 'V'), ('với', 'E'), ('anh', 'Nc'), ('ta', 'P'), ('Đừng', 'Np'), ('làm vậy', 'V'), ('nữa', 'R'), ('cô', 'Nc'), ('ấy', 'P'), ('kể', 'V'), ('lại', 'V')]\n",
            "--------------------------------------------------\n",
            "Original: Dữ liệu đến từ nhiều nguồn: báo chí, mạng xã hội, và thậm chí cả tin đồn.\n",
            "Preprocessed: Dữ liệu đến từ nhiều nguồn báo chí mạng xã hội và thậm chí cả tin đồn\n",
            "Diacritics: Du lieu den tu nhieu nguon bao chi mang xa hoi va tham chi ca tin don\n",
            "Tokens: ['Dữ liệu', 'đến', 'từ', 'nhiều', 'nguồn', 'báo chí mạng', 'xã hội', 'và', 'thậm chí', 'cả', 'tin đồn']\n",
            "POS Tags: [('Dữ liệu', 'N'), ('đến', 'V'), ('từ', 'E'), ('nhiều', 'A'), ('nguồn', 'N'), ('báo chí mạng', 'N'), ('xã hội', 'N'), ('và', 'C'), ('thậm chí', 'R'), ('cả', 'T'), ('tin đồn', 'M')]\n",
            "--------------------------------------------------\n",
            "Original: Hệ thống này hoạt động tốt ở cả thành thị lẫn nông thôn -- điều này rất quan trọng.\n",
            "Preprocessed: Hệ thống này hoạt động tốt ở cả thành thị lẫn nông thôn điều này rất quan trọng\n",
            "Diacritics: He thong nay hoat dong tot o ca thanh thi lan nong thon dieu nay rat quan trong\n",
            "Tokens: ['Hệ thống', 'này', 'hoạt động', 'tốt', 'ở', 'cả', 'thành thị', 'lẫn', 'nông thôn', 'điều', 'này', 'rất', 'quan trọng']\n",
            "POS Tags: [('Hệ thống', 'N'), ('này', 'P'), ('hoạt động', 'V'), ('tốt', 'A'), ('ở', 'E'), ('cả', 'P'), ('thành thị', 'N'), ('lẫn', 'C'), ('nông thôn', 'N'), ('điều', 'N'), ('này', 'P'), ('rất', 'R'), ('quan trọng', 'A')]\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline for conversion from Vietnamese to Sign language"
      ],
      "metadata": {
        "id": "RE2m3fyvDcjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "PhFw3aS0foTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from underthesea import sent_tokenize\n",
        "\n",
        "def preprocess(text):\n",
        "    # Remove special characters (except Vietnamese accents and common punctuation)\n",
        "    text = re.sub(r'[^\\w\\sÀ-ỹ,.!?]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Example\n",
        "text = \"Tôi   đã ăn   cơm   rồi!!! Bạn thì sao???\"\n",
        "output = preprocess(text)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPSKfUaIDkbU",
        "outputId": "ae4ae95c-b20c-47fc-8ffc-89e289dc1a68"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tôi đã ăn cơm rồi!!! Bạn thì sao???\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove stopwords"
      ],
      "metadata": {
        "id": "460MREAlf1Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvi import ViTokenizer\n",
        "import os\n",
        "\n",
        "stopwords = set()\n",
        "with open(\"/content/vietnamese-stopwords.txt\", encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        stopwords.add(line.strip())\n",
        "\n",
        "emotion_words = {'vui', 'buồn', 'tức giận', 'lo lắng', 'nhé', 'đấy'} # Need to expand to more emotion words\n",
        "\n",
        "def remove_stopwords(sentence):\n",
        "    words = ViTokenizer.tokenize(sentence).split()\n",
        "    content_words = [w for w in words if w.replace('_', ' ') not in stopwords]\n",
        "    emotion_tags = [w for w in words if w.replace('_', ' ') in emotion_words]\n",
        "    return content_words, emotion_tags\n",
        "\n",
        "# Example\n",
        "filtered, emotion = remove_stopwords(output)\n",
        "print(\"Filtered:\", filtered)\n",
        "print(\"Emotion Words:\", emotion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjr6ilz3f3Wq",
        "outputId": "fdbfbdc3-3a91-4ecb-b2fb-da739e2c9314"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered: ['Tôi', 'cơm', '!', '!', '!', 'Bạn', '?', '?', '?']\n",
            "Emotion Words: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change words order TODO"
      ],
      "metadata": {
        "id": "Kt1jT9TYhL_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization + POS tagging"
      ],
      "metadata": {
        "id": "0f18YPxniD3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from underthesea import pos_tag, word_tokenize\n",
        "\n",
        "def tokenize_and_pos_tag(text):\n",
        "    tokens = word_tokenize(text, format=\"text\")\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    return pos_tags\n",
        "\n",
        "example_text = \"Đây là một ví dụ về việc phân tích từ loại trong tiếng Việt.\"\n",
        "result = tokenize_and_pos_tag(example_text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3oXQgj9iGbY",
        "outputId": "0708a2fc-6c32-44a4-c167-f99aa1b5c948"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Đây', 'P'), ('là', 'V'), ('một', 'M'), ('ví_dụ', 'N'), ('về', 'E'), ('việc', 'N'), ('phân_tích', 'V'), ('từ_loại', 'N'), ('trong', 'E'), ('tiếng', 'N'), ('Việt', 'Np'), ('.', 'CH')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert words to gloss"
      ],
      "metadata": {
        "id": "M_WBj68GhvqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gloss_dict = {\n",
        "    \"ôi_trời_ơi\": \"OH_MY_GOD\",\n",
        "    \"hôm_nay\": \"TODAY\",\n",
        "    \"tôi\": \"I\",\n",
        "    \"quá\": \"VERY\",\n",
        "    \"mệt_mỏi\": \"TIRED\",\n",
        "    \"vì\": \"BECAUSE\",\n",
        "    \"phải\": \"HAVE_TO\",\n",
        "    \"làm_việc\": \"WORK\",\n",
        "    \"đến\": \"UNTIL\",\n",
        "    \"tận\": \"VERY\",\n",
        "    \"khuya\": \"LATE_NIGHT\"\n",
        "}\n",
        "\n",
        "def convert_to_gloss(pos_tags):\n",
        "    gloss_sentence = []\n",
        "    for word, pos in pos_tags:\n",
        "        gloss = gloss_dict.get(word, word.upper())\n",
        "        gloss_sentence.append(gloss)\n",
        "    return gloss_sentence"
      ],
      "metadata": {
        "id": "OgnPqOjBibpn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add facial expression tags"
      ],
      "metadata": {
        "id": "VHFa6sC-igFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_facial_expression(gloss_sentence):\n",
        "    final_output = []\n",
        "    for word in gloss_sentence:\n",
        "        # Emotion-based expression\n",
        "        if word in [\"OH_MY_GOD\"]:\n",
        "            final_output.append(\"[SURPRISED EXPRESSION]\")\n",
        "        elif word in [\"VERY\"]:\n",
        "            final_output.append(\"[INTENSITY FACIAL MARKER]\")\n",
        "        elif word in [\"TIRED\"]:\n",
        "            final_output.append(\"[SAD EXPRESSION]\")\n",
        "        elif word in [\"BECAUSE\"]:\n",
        "            final_output.append(\"[REASONING EXPRESSION]\")\n",
        "        final_output.append(word)\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "f8PUi0oEiiqV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate output"
      ],
      "metadata": {
        "id": "rcskZmgIilj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_output(text):\n",
        "    print(\"Original:\", text)\n",
        "    text = preprocess(text)\n",
        "    print(\"Preprocessed:\", text)\n",
        "\n",
        "    tagged = tokenize_and_pos_tag(text)\n",
        "    print(\"POS Tagged:\", tagged)\n",
        "\n",
        "    gloss = convert_to_gloss(tagged)\n",
        "    print(\"Gloss:\", gloss)\n",
        "\n",
        "    gloss_with_expr = add_facial_expression(gloss)\n",
        "    print(\"Final with facial expressions:\", gloss_with_expr)\n",
        "\n",
        "    return ' '.join(gloss_with_expr)"
      ],
      "metadata": {
        "id": "wtu828Xfim9C"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"Ôi trời ơi! Hôm nay tôi quá mệt mỏi vì phải làm việc đến tận khuya.\"\n",
        "\n",
        "print(\"\\nGenerated Gloss Output:\\n\")\n",
        "result = generate_output(example_text)\n",
        "print(\"\\nFinal Output Text:\\n\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZiUq7Koi2Kx",
        "outputId": "915405c5-d840-450b-b11a-c492b1988186"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Gloss Output:\n",
            "\n",
            "Original: Ôi trời ơi! Hôm nay tôi quá mệt mỏi vì phải làm việc đến tận khuya.\n",
            "Preprocessed: Ôi trời ơi! Hôm nay tôi quá mệt mỏi vì phải làm việc đến tận khuya.\n",
            "POS Tagged: [('Ôi', 'I'), ('trời_ơi', 'N'), ('! Hôm_nay', 'N'), ('tôi', 'P'), ('quá', 'R'), ('mệt_mỏi', 'A'), ('vì', 'E'), ('phải', 'V'), ('làm_việc', 'V'), ('đến', 'E'), ('tận', 'T'), ('khuya', 'A'), ('.', 'CH')]\n",
            "Gloss: ['ÔI', 'TRỜI_ƠI', '! HÔM_NAY', 'I', 'VERY', 'TIRED', 'BECAUSE', 'HAVE_TO', 'WORK', 'UNTIL', 'VERY', 'LATE_NIGHT', '.']\n",
            "Final with facial expressions: ['ÔI', 'TRỜI_ƠI', '! HÔM_NAY', 'I', '[INTENSITY FACIAL MARKER]', 'VERY', '[SAD EXPRESSION]', 'TIRED', '[REASONING EXPRESSION]', 'BECAUSE', 'HAVE_TO', 'WORK', 'UNTIL', '[INTENSITY FACIAL MARKER]', 'VERY', 'LATE_NIGHT', '.']\n",
            "\n",
            "Final Output Text:\n",
            " ÔI TRỜI_ƠI ! HÔM_NAY I [INTENSITY FACIAL MARKER] VERY [SAD EXPRESSION] TIRED [REASONING EXPRESSION] BECAUSE HAVE_TO WORK UNTIL [INTENSITY FACIAL MARKER] VERY LATE_NIGHT .\n"
          ]
        }
      ]
    }
  ]
}